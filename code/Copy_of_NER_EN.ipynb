{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA21Jo5d9SVq"
      },
      "source": [
        "\n",
        "\n",
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_EN.ipynb)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzIdjHkAW8TB"
      },
      "source": [
        "# **Detect entities in English text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIeCOiJNW-88"
      },
      "source": [
        "## 1. Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGJktFHdHL1n",
        "outputId": "e8e35324-95e1-445b-a408-5116a5a92061"
      },
      "outputs": [],
      "source": [
        "# Install PySpark and Spark NLP\n",
        "! pip install -q pyspark==3.1.2 spark-nlp\n",
        "\n",
        "# Install Spark NLP Display lib\n",
        "! pip install --upgrade -q spark-nlp-display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCIT5VLxS3I1"
      },
      "source": [
        "## 2. Start the Spark session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khjM-z9ORFU3"
      },
      "source": [
        "Import dependencies and start Spark session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# skip this cell!\n",
        "import sys\n",
        "import os\n",
        "\n",
        "os.environ['HADOOP_HOME'] = \"S:\\hadoop-3.2.3\"\n",
        "#os.environ['HADOOP_HOME'] = \"S:\\spark-3.2.1-bin-hadoop3.2\"\n",
        "#os.environ['HADOOP_HOME'] = \"C:/Mine/Spark/hadoop-2.6.0\"\n",
        "sys.path.append(\"S:/hadoop-3.2.3/bin\")\n",
        "#sys.path.append(\"C:/Mine/Spark/hadoop-2.6.0/bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "sw-t1zxlHTB7"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1120)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1106)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:563)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1627)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$13(SparkContext.scala:508)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$13$adapted(SparkContext.scala:508)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:508)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1814)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1791)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:302)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:326)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:343)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)\r\n\t... 21 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Nino\\Documents\\NLP\\NLP-Untitled\\code\\Copy_of_NER_EN.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nino/Documents/NLP/NLP-Untitled/code/Copy_of_NER_EN.ipynb#ch0000006?line=15'>16</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msparknlp\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nino/Documents/NLP/NLP-Untitled/code/Copy_of_NER_EN.ipynb#ch0000006?line=16'>17</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msparknlp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpretrained\u001b[39;00m \u001b[39mimport\u001b[39;00m PretrainedPipeline\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nino/Documents/NLP/NLP-Untitled/code/Copy_of_NER_EN.ipynb#ch0000006?line=18'>19</a>\u001b[0m spark \u001b[39m=\u001b[39m sparknlp\u001b[39m.\u001b[39;49mstart()\n",
            "File \u001b[1;32mS:\\Python\\lib\\site-packages\\sparknlp\\__init__.py:273\u001b[0m, in \u001b[0;36mstart\u001b[1;34m(gpu, spark23, spark24, spark32, memory, cache_folder, log_folder, cluster_tmp_dir, real_time_output, output_level)\u001b[0m\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/sparknlp/__init__.py?line=270'>271</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m SparkRealTimeOutput()\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/sparknlp/__init__.py?line=271'>272</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///s%3A/Python/lib/site-packages/sparknlp/__init__.py?line=272'>273</a>\u001b[0m     spark_session \u001b[39m=\u001b[39m start_without_realtime_output()\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/sparknlp/__init__.py?line=273'>274</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m spark_session\n",
            "File \u001b[1;32mS:\\Python\\lib\\site-packages\\sparknlp\\__init__.py:177\u001b[0m, in \u001b[0;36mstart.<locals>.start_without_realtime_output\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/sparknlp/__init__.py?line=173'>174</a>\u001b[0m \u001b[39mif\u001b[39;00m cluster_tmp_dir \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/sparknlp/__init__.py?line=174'>175</a>\u001b[0m     builder\u001b[39m.\u001b[39mconfig(\u001b[39m\"\u001b[39m\u001b[39mspark.jsl.settings.storage.cluster_tmp_dir\u001b[39m\u001b[39m\"\u001b[39m, cluster_tmp_dir)\n\u001b[1;32m--> <a href='file:///s%3A/Python/lib/site-packages/sparknlp/__init__.py?line=176'>177</a>\u001b[0m \u001b[39mreturn\u001b[39;00m builder\u001b[39m.\u001b[39;49mgetOrCreate()\n",
            "File \u001b[1;32mS:\\Python\\lib\\site-packages\\pyspark\\sql\\session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/sql/session.py?line=225'>226</a>\u001b[0m         sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/sql/session.py?line=226'>227</a>\u001b[0m     \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///s%3A/Python/lib/site-packages/pyspark/sql/session.py?line=227'>228</a>\u001b[0m     sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/sql/session.py?line=228'>229</a>\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/sql/session.py?line=229'>230</a>\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/sql/session.py?line=230'>231</a>\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc)\n",
            "File \u001b[1;32mS:\\Python\\lib\\site-packages\\pyspark\\context.py:384\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/context.py?line=381'>382</a>\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/context.py?line=382'>383</a>\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///s%3A/Python/lib/site-packages/pyspark/context.py?line=383'>384</a>\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/context.py?line=384'>385</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
            "File \u001b[1;32mS:\\Python\\lib\\site-packages\\pyspark\\context.py:146\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/context.py?line=143'>144</a>\u001b[0m SparkContext\u001b[39m.\u001b[39m_ensure_initialized(\u001b[39mself\u001b[39m, gateway\u001b[39m=\u001b[39mgateway, conf\u001b[39m=\u001b[39mconf)\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/context.py?line=144'>145</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///s%3A/Python/lib/site-packages/pyspark/context.py?line=145'>146</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/context.py?line=146'>147</a>\u001b[0m                   conf, jsc, profiler_cls)\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/context.py?line=147'>148</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/context.py?line=148'>149</a>\u001b[0m     \u001b[39m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/context.py?line=149'>150</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop()\n",
            "File \u001b[1;32mS:\\Python\\lib\\site-packages\\pyspark\\context.py:209\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/context.py?line=205'>206</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironment[\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/context.py?line=207'>208</a>\u001b[0m \u001b[39m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> <a href='file:///s%3A/Python/lib/site-packages/pyspark/context.py?line=208'>209</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc \u001b[39m=\u001b[39m jsc \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize_context(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conf\u001b[39m.\u001b[39;49m_jconf)\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/context.py?line=209'>210</a>\u001b[0m \u001b[39m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/context.py?line=210'>211</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conf \u001b[39m=\u001b[39m SparkConf(_jconf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc()\u001b[39m.\u001b[39mconf())\n",
            "File \u001b[1;32mS:\\Python\\lib\\site-packages\\pyspark\\context.py:321\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/context.py?line=316'>317</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_initialize_context\u001b[39m(\u001b[39mself\u001b[39m, jconf):\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/context.py?line=317'>318</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/context.py?line=318'>319</a>\u001b[0m \u001b[39m    Initialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/pyspark/context.py?line=319'>320</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///s%3A/Python/lib/site-packages/pyspark/context.py?line=320'>321</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mJavaSparkContext(jconf)\n",
            "File \u001b[1;32mS:\\Python\\lib\\site-packages\\py4j\\java_gateway.py:1568\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   <a href='file:///s%3A/Python/lib/site-packages/py4j/java_gateway.py?line=1561'>1562</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   <a href='file:///s%3A/Python/lib/site-packages/py4j/java_gateway.py?line=1562'>1563</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_command_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   <a href='file:///s%3A/Python/lib/site-packages/py4j/java_gateway.py?line=1563'>1564</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   <a href='file:///s%3A/Python/lib/site-packages/py4j/java_gateway.py?line=1564'>1565</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   <a href='file:///s%3A/Python/lib/site-packages/py4j/java_gateway.py?line=1566'>1567</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> <a href='file:///s%3A/Python/lib/site-packages/py4j/java_gateway.py?line=1567'>1568</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   <a href='file:///s%3A/Python/lib/site-packages/py4j/java_gateway.py?line=1568'>1569</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client, \u001b[39mNone\u001b[39;49;00m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fqn)\n\u001b[0;32m   <a href='file:///s%3A/Python/lib/site-packages/py4j/java_gateway.py?line=1570'>1571</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   <a href='file:///s%3A/Python/lib/site-packages/py4j/java_gateway.py?line=1571'>1572</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
            "File \u001b[1;32mS:\\Python\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/py4j/protocol.py?line=323'>324</a>\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/py4j/protocol.py?line=324'>325</a>\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> <a href='file:///s%3A/Python/lib/site-packages/py4j/protocol.py?line=325'>326</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/py4j/protocol.py?line=326'>327</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/py4j/protocol.py?line=327'>328</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/py4j/protocol.py?line=328'>329</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/py4j/protocol.py?line=329'>330</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/py4j/protocol.py?line=330'>331</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/py4j/protocol.py?line=331'>332</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1120)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1106)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:563)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1627)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$13(SparkContext.scala:508)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$13$adapted(SparkContext.scala:508)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:508)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1814)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1791)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:302)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:326)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:343)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)\r\n\t... 21 more\r\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "os.environ['HADOOP_HOME'] = \"S:\\spark-3.1.2-bin-hadoop3.2\"\n",
        "#os.environ['HADOOP_HOME'] = \"S:\\spark-3.2.1-bin-hadoop3.2\"\n",
        "#os.environ['HADOOP_HOME'] = \"C:/Mine/Spark/hadoop-2.6.0\"\n",
        "sys.path.append(\"S:/spark-3.1.2-bin-hadoop3.2/bin\")\n",
        "#sys.path.append(\"C:/Mine/Spark/hadoop-2.6.0/bin\")\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "\n",
        "spark = sparknlp.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RgiqfX5XDqb"
      },
      "source": [
        "## 3. Select the DL model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLuDz_t40be4"
      },
      "outputs": [],
      "source": [
        "# If you change the model, re-run all the cells below\n",
        "# Other applicable models: ner_dl, ner_dl_bert\n",
        "MODEL_NAME = \"onto_100\"\n",
        "#MODEL_NAME = \"ner_dl\"\n",
        "#MODEL_NAME = \"ner_dl_bert\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4ZeZ2NSbOlG",
        "outputId": "245d3d03-e5e4-4a71-8241-ac2c1880334a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "glove_100d download started this may take some time.\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute '_jvm'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Nino\\Documents\\NLP\\NLP-Untitled\\code\\Copy_of_NER_EN.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nino/Documents/NLP/NLP-Untitled/code/Copy_of_NER_EN.ipynb#ch0000009?line=0'>1</a>\u001b[0m \u001b[39m# ner_dl and onto_100 model are trained with glove_100d, so the embeddings in\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nino/Documents/NLP/NLP-Untitled/code/Copy_of_NER_EN.ipynb#ch0000009?line=1'>2</a>\u001b[0m   \u001b[39m# the pipeline should match\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nino/Documents/NLP/NLP-Untitled/code/Copy_of_NER_EN.ipynb#ch0000009?line=2'>3</a>\u001b[0m \u001b[39mif\u001b[39;00m (MODEL_NAME \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mner_dl\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m (MODEL_NAME \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39monto_100\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Nino/Documents/NLP/NLP-Untitled/code/Copy_of_NER_EN.ipynb#ch0000009?line=3'>4</a>\u001b[0m     embeddings \u001b[39m=\u001b[39m WordEmbeddingsModel\u001b[39m.\u001b[39;49mpretrained(\u001b[39m'\u001b[39;49m\u001b[39mglove_100d\u001b[39;49m\u001b[39m'\u001b[39;49m) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nino/Documents/NLP/NLP-Untitled/code/Copy_of_NER_EN.ipynb#ch0000009?line=4'>5</a>\u001b[0m         \u001b[39m.\u001b[39msetInputCols([\u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m'\u001b[39m]) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nino/Documents/NLP/NLP-Untitled/code/Copy_of_NER_EN.ipynb#ch0000009?line=5'>6</a>\u001b[0m         \u001b[39m.\u001b[39msetOutputCol(\u001b[39m\"\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nino/Documents/NLP/NLP-Untitled/code/Copy_of_NER_EN.ipynb#ch0000009?line=7'>8</a>\u001b[0m \u001b[39m# Bert model uses Bert embeddings\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nino/Documents/NLP/NLP-Untitled/code/Copy_of_NER_EN.ipynb#ch0000009?line=8'>9</a>\u001b[0m \u001b[39melif\u001b[39;00m MODEL_NAME \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mner_dl_bert\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "File \u001b[1;32mS:\\Python\\lib\\site-packages\\sparknlp\\annotator.py:6416\u001b[0m, in \u001b[0;36mWordEmbeddingsModel.pretrained\u001b[1;34m(name, lang, remote_loc)\u001b[0m\n\u001b[0;32m   <a href='file:///s%3A/Python/lib/site-packages/sparknlp/annotator.py?line=6397'>6398</a>\u001b[0m \u001b[39m\"\"\"Downloads and loads a pretrained model.\u001b[39;00m\n\u001b[0;32m   <a href='file:///s%3A/Python/lib/site-packages/sparknlp/annotator.py?line=6398'>6399</a>\u001b[0m \n\u001b[0;32m   <a href='file:///s%3A/Python/lib/site-packages/sparknlp/annotator.py?line=6399'>6400</a>\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///s%3A/Python/lib/site-packages/sparknlp/annotator.py?line=6412'>6413</a>\u001b[0m \u001b[39m    The restored model\u001b[39;00m\n\u001b[0;32m   <a href='file:///s%3A/Python/lib/site-packages/sparknlp/annotator.py?line=6413'>6414</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///s%3A/Python/lib/site-packages/sparknlp/annotator.py?line=6414'>6415</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msparknlp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpretrained\u001b[39;00m \u001b[39mimport\u001b[39;00m ResourceDownloader\n\u001b[1;32m-> <a href='file:///s%3A/Python/lib/site-packages/sparknlp/annotator.py?line=6415'>6416</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ResourceDownloader\u001b[39m.\u001b[39;49mdownloadModel(WordEmbeddingsModel, name, lang, remote_loc)\n",
            "File \u001b[1;32mS:\\Python\\lib\\site-packages\\sparknlp\\pretrained.py:50\u001b[0m, in \u001b[0;36mResourceDownloader.downloadModel\u001b[1;34m(reader, name, language, remote_loc, j_dwn)\u001b[0m\n\u001b[0;32m     <a href='file:///s%3A/Python/lib/site-packages/sparknlp/pretrained.py?line=46'>47</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m     <a href='file:///s%3A/Python/lib/site-packages/sparknlp/pretrained.py?line=47'>48</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownloadModel\u001b[39m(reader, name, language, remote_loc\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, j_dwn\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPythonResourceDownloader\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     <a href='file:///s%3A/Python/lib/site-packages/sparknlp/pretrained.py?line=48'>49</a>\u001b[0m     \u001b[39mprint\u001b[39m(name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m download started this may take some time.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='file:///s%3A/Python/lib/site-packages/sparknlp/pretrained.py?line=49'>50</a>\u001b[0m     file_size \u001b[39m=\u001b[39m _internal\u001b[39m.\u001b[39;49m_GetResourceSize(name, language, remote_loc)\u001b[39m.\u001b[39mapply()\n\u001b[0;32m     <a href='file:///s%3A/Python/lib/site-packages/sparknlp/pretrained.py?line=50'>51</a>\u001b[0m     \u001b[39mif\u001b[39;00m file_size \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m-1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     <a href='file:///s%3A/Python/lib/site-packages/sparknlp/pretrained.py?line=51'>52</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCan not find the model to download please check the name!\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mS:\\Python\\lib\\site-packages\\sparknlp\\internal.py:234\u001b[0m, in \u001b[0;36m_GetResourceSize.__init__\u001b[1;34m(self, name, language, remote_loc)\u001b[0m\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/sparknlp/internal.py?line=232'>233</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, language, remote_loc):\n\u001b[1;32m--> <a href='file:///s%3A/Python/lib/site-packages/sparknlp/internal.py?line=233'>234</a>\u001b[0m     \u001b[39msuper\u001b[39;49m(_GetResourceSize, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/sparknlp/internal.py?line=234'>235</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mcom.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.getDownloadSize\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, language, remote_loc)\n",
            "File \u001b[1;32mS:\\Python\\lib\\site-packages\\sparknlp\\internal.py:165\u001b[0m, in \u001b[0;36mExtendedJavaWrapper.__init__\u001b[1;34m(self, java_obj, *args)\u001b[0m\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/sparknlp/internal.py?line=162'>163</a>\u001b[0m \u001b[39msuper\u001b[39m(ExtendedJavaWrapper, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(java_obj)\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/sparknlp/internal.py?line=163'>164</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n\u001b[1;32m--> <a href='file:///s%3A/Python/lib/site-packages/sparknlp/internal.py?line=164'>165</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnew_java_obj(java_obj, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/sparknlp/internal.py?line=165'>166</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj\n",
            "File \u001b[1;32mS:\\Python\\lib\\site-packages\\sparknlp\\internal.py:175\u001b[0m, in \u001b[0;36mExtendedJavaWrapper.new_java_obj\u001b[1;34m(self, java_class, *args)\u001b[0m\n\u001b[0;32m    <a href='file:///s%3A/Python/lib/site-packages/sparknlp/internal.py?line=173'>174</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnew_java_obj\u001b[39m(\u001b[39mself\u001b[39m, java_class, \u001b[39m*\u001b[39margs):\n\u001b[1;32m--> <a href='file:///s%3A/Python/lib/site-packages/sparknlp/internal.py?line=174'>175</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_java_obj(java_class, \u001b[39m*\u001b[39;49margs)\n",
            "File \u001b[1;32mS:\\Python\\lib\\site-packages\\pyspark\\ml\\wrapper.py:65\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[1;34m(java_class, *args)\u001b[0m\n\u001b[0;32m     <a href='file:///s%3A/Python/lib/site-packages/pyspark/ml/wrapper.py?line=62'>63</a>\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m java_class\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     <a href='file:///s%3A/Python/lib/site-packages/pyspark/ml/wrapper.py?line=63'>64</a>\u001b[0m     java_obj \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(java_obj, name)\n\u001b[1;32m---> <a href='file:///s%3A/Python/lib/site-packages/pyspark/ml/wrapper.py?line=64'>65</a>\u001b[0m java_args \u001b[39m=\u001b[39m [_py2java(sc, arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args]\n\u001b[0;32m     <a href='file:///s%3A/Python/lib/site-packages/pyspark/ml/wrapper.py?line=65'>66</a>\u001b[0m \u001b[39mreturn\u001b[39;00m java_obj(\u001b[39m*\u001b[39mjava_args)\n",
            "File \u001b[1;32mS:\\Python\\lib\\site-packages\\pyspark\\ml\\wrapper.py:65\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///s%3A/Python/lib/site-packages/pyspark/ml/wrapper.py?line=62'>63</a>\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m java_class\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     <a href='file:///s%3A/Python/lib/site-packages/pyspark/ml/wrapper.py?line=63'>64</a>\u001b[0m     java_obj \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(java_obj, name)\n\u001b[1;32m---> <a href='file:///s%3A/Python/lib/site-packages/pyspark/ml/wrapper.py?line=64'>65</a>\u001b[0m java_args \u001b[39m=\u001b[39m [_py2java(sc, arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args]\n\u001b[0;32m     <a href='file:///s%3A/Python/lib/site-packages/pyspark/ml/wrapper.py?line=65'>66</a>\u001b[0m \u001b[39mreturn\u001b[39;00m java_obj(\u001b[39m*\u001b[39mjava_args)\n",
            "File \u001b[1;32mS:\\Python\\lib\\site-packages\\pyspark\\ml\\common.py:81\u001b[0m, in \u001b[0;36m_py2java\u001b[1;34m(sc, obj)\u001b[0m\n\u001b[0;32m     <a href='file:///s%3A/Python/lib/site-packages/pyspark/ml/common.py?line=78'>79</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///s%3A/Python/lib/site-packages/pyspark/ml/common.py?line=79'>80</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(PickleSerializer()\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m---> <a href='file:///s%3A/Python/lib/site-packages/pyspark/ml/common.py?line=80'>81</a>\u001b[0m     obj \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39morg\u001b[39m.\u001b[39mapache\u001b[39m.\u001b[39mspark\u001b[39m.\u001b[39mml\u001b[39m.\u001b[39mpython\u001b[39m.\u001b[39mMLSerDe\u001b[39m.\u001b[39mloads(data)\n\u001b[0;32m     <a href='file:///s%3A/Python/lib/site-packages/pyspark/ml/common.py?line=81'>82</a>\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_jvm'"
          ]
        }
      ],
      "source": [
        "# ner_dl and onto_100 model are trained with glove_100d, so the embeddings in\n",
        "  # the pipeline should match\n",
        "if (MODEL_NAME == \"ner_dl\") or (MODEL_NAME == \"onto_100\"):\n",
        "    embeddings = WordEmbeddingsModel.pretrained('glove_100d') \\\n",
        "        .setInputCols([\"document\", 'token']) \\\n",
        "        .setOutputCol(\"embeddings\")\n",
        "\n",
        "# Bert model uses Bert embeddings\n",
        "elif MODEL_NAME == \"ner_dl_bert\":\n",
        "    embeddings = BertEmbeddings.pretrained(name='bert_base_cased', lang='en') \\\n",
        "        .setInputCols(['document', 'token']) \\\n",
        "        .setOutputCol('embeddings')\n",
        "\n",
        "ner_model = NerDLModel.pretrained(MODEL_NAME, 'en') \\\n",
        "      .setInputCols(['document', 'token', 'embeddings']) \\\n",
        "      .setOutputCol('ner')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y9GpdJhXIpD"
      },
      "source": [
        "## 4. Some sample examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LvO-LPZZWUcO",
        "outputId": "10e861a7-2f75-47e5-e54f-8e5966bf9b3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "from sparknlp_display import NerVisualizer\n",
        "i = 0\n",
        "empty_dict_ct = 0\n",
        "if MODEL_NAME == \"onto_100\": # short stories - 26 without entities  \n",
        "  for file in glob.iglob('../material/medium_stories/*.txt'): \n",
        "    print(file)\n",
        "    f = open(file, \"r\")\n",
        "    text_list = [f.read()]\n",
        "\n",
        "    documentAssembler = DocumentAssembler() \\\n",
        "      .setInputCol('text') \\\n",
        "      .setOutputCol('document')\n",
        "\n",
        "    tokenizer = Tokenizer() \\\n",
        "        .setInputCols(['document']) \\\n",
        "        .setOutputCol('token')\n",
        "\n",
        "    ner_converter = NerConverter() \\\n",
        "        .setInputCols(['document', 'token', 'ner']) \\\n",
        "        .setOutputCol('ner_chunk')\n",
        "\n",
        "    nlp_pipeline = Pipeline(stages=[\n",
        "        documentAssembler, \n",
        "        tokenizer,\n",
        "        embeddings,\n",
        "        ner_model,\n",
        "        ner_converter\n",
        "    ])\n",
        "    \n",
        "    empty_df = spark.createDataFrame([['']]).toDF('text')\n",
        "    pipeline_model = nlp_pipeline.fit(empty_df)\n",
        "    df = spark.createDataFrame(pd.DataFrame({'text': text_list}))\n",
        "    result = pipeline_model.transform(df)\n",
        "\n",
        "\n",
        "    rows = result.select(\"ner\").collect()[0]\n",
        "    #entities = set()\n",
        "    entities_dict = {}\n",
        "\n",
        "    for row in rows[0]: # BERT - without rows[0]\n",
        "      #print(row)\n",
        "      #print(row.result)\n",
        "      if(row.result.endswith('PERSON')):# BERT - PER\n",
        "        if(row.metadata[\"word\"] not in entities_dict):\n",
        "          entities_dict[row.metadata[\"word\"]] = 1\n",
        "        else:\n",
        "          entities_dict[row.metadata[\"word\"]] += 1\n",
        "          #print(row.metadata[\"word\"])\n",
        "          #entities.add(row.metadata[\"word\"])\n",
        "\n",
        "    #print(entities_dict)\n",
        "    curr_entities = (dict((k, v) for k, v in entities_dict.items() if v >= 2))\n",
        "    if not curr_entities:\n",
        "      empty_dict_ct += 1\n",
        "    #print(dict((k, v) for k, v in entities_dict.items() if v >= 2))\n",
        "\n",
        "    #print(result.select(\"ner\").collect())\n",
        "    NerVisualizer().display(\n",
        "        result = result.collect()[0],\n",
        "        label_col = 'ner_chunk',\n",
        "        document_col = 'document'\n",
        "    )\n",
        "\n",
        "    i = i + 1\n",
        "    if i>0: break\n",
        "\n",
        "  print(empty_dict_ct)\n",
        "if MODEL_NAME == \"ner_dl_bert\": # short stories - 12 without entities\n",
        "  for file in glob.iglob('../material/medium_stories/*.txt'): #('./*.txt')\n",
        "    \n",
        "    #file = \"/content/sample_data/medium_stories/The Most Dangerous Game_modified.txt\"\n",
        "    print(file)\n",
        "    f = open(file, \"r\")\n",
        "    text_list = [f.read()]\n",
        "\n",
        "    documentAssembler = DocumentAssembler() \\\n",
        "      .setInputCol('text') \\\n",
        "      .setOutputCol('document')\n",
        "\n",
        "    tokenizer = Tokenizer() \\\n",
        "        .setInputCols(['document']) \\\n",
        "        .setOutputCol('token')\n",
        "\n",
        "    ner_converter = NerConverter() \\\n",
        "        .setInputCols(['document', 'token', 'ner']) \\\n",
        "        .setOutputCol('ner_chunk')\n",
        "\n",
        "    nlp_pipeline = Pipeline(stages=[\n",
        "        documentAssembler, \n",
        "        tokenizer,\n",
        "        embeddings,\n",
        "        ner_model,\n",
        "        ner_converter\n",
        "    ])\n",
        "    \n",
        "    empty_df = spark.createDataFrame([['']]).toDF('text')\n",
        "    pipeline_model = nlp_pipeline.fit(empty_df)\n",
        "    df = spark.createDataFrame(pd.DataFrame({'text': text_list}))\n",
        "    result = pipeline_model.transform(df)\n",
        "\n",
        "\n",
        "    rows = result.select(\"ner\").collect()[0]\n",
        "    #entities = set()\n",
        "    entities_dict = {}\n",
        "\n",
        "    for row in rows[0]: # BERT - without rows[0]\n",
        "      #print(row)\n",
        "      #print(row.result)\n",
        "      if(row.result.endswith('PER') or row.result.endswith('MISC') or row.result.endswith('ORG')):# BERT - PER\n",
        "        if(row.metadata[\"word\"] not in entities_dict):\n",
        "          entities_dict[row.metadata[\"word\"]] = 1\n",
        "        else:\n",
        "          entities_dict[row.metadata[\"word\"]] += 1\n",
        "          #print(row.metadata[\"word\"])\n",
        "          #entities.add(row.metadata[\"word\"])\n",
        "\n",
        "    #print(entities_dict)\n",
        "    curr_entities = (dict((k, v) for k, v in entities_dict.items() if v >= 2))\n",
        "    if not curr_entities:\n",
        "      empty_dict_ct += 1\n",
        "    print(dict((k, v) for k, v in entities_dict.items() if v >= 2))\n",
        "\n",
        "    print(result.select(\"ner\").collect())\n",
        "    NerVisualizer().display(\n",
        "        result = result.collect()[0],\n",
        "        label_col = 'ner_chunk',\n",
        "        document_col = 'document'\n",
        "    )\n",
        "\n",
        "    i = i + 1\n",
        "    if i>0: break\n",
        "\n",
        "  print(empty_dict_ct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO2rA3ZbOclq",
        "outputId": "c870e2db-a7ba-4e61-bb3c-fb26d6a600fa"
      },
      "outputs": [],
      "source": [
        "cd drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psL510rnOQtI",
        "outputId": "9c53952e-5fb4-4a6c-a7cf-bb5c97896e7f"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0z10h5FX_BH"
      },
      "outputs": [],
      "source": [
        "empty_df = spark.createDataFrame([['']]).toDF('text')\n",
        "pipeline_model = nlp_pipeline.fit(empty_df)\n",
        "df = spark.createDataFrame(pd.DataFrame({'text': text_list}))\n",
        "result = pipeline_model.transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBOKkB2THdGI"
      },
      "outputs": [],
      "source": [
        "text_list = [\n",
        "    \"\"\"William Henry Gates III (born October 28, 1955) is an American business magnate, software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft, Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect, while also being the largest individual shareholder until May 2014. He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico; it went on to become the world's largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect. During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time role at Microsoft and full-time work at the Bill & Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.[9] He gradually transferred his duties to Ray Ozzie and Craig Mundie. He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella.\"\"\",\n",
        "    \"\"\"The Mona Lisa is a 16th century oil painting created by Leonardo. It's held at the Louvre in Paris.\"\"\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XftYgju4XOw_"
      },
      "source": [
        "## 5. Define Spark NLP pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBggF5P8J1gc",
        "outputId": "c6c9afc1-39df-4221-b40c-93d0f48b13f6"
      },
      "outputs": [],
      "source": [
        "documentAssembler = DocumentAssembler() \\\n",
        "    .setInputCol('text') \\\n",
        "    .setOutputCol('document')\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols(['document']) \\\n",
        "    .setOutputCol('token')\n",
        "\n",
        "# ner_dl and onto_100 model are trained with glove_100d, so the embeddings in\n",
        "# the pipeline should match\n",
        "if (MODEL_NAME == \"ner_dl\") or (MODEL_NAME == \"onto_100\"):\n",
        "    embeddings = WordEmbeddingsModel.pretrained('glove_100d') \\\n",
        "        .setInputCols([\"document\", 'token']) \\\n",
        "        .setOutputCol(\"embeddings\")\n",
        "\n",
        "# Bert model uses Bert embeddings\n",
        "elif MODEL_NAME == \"ner_dl_bert\":\n",
        "    embeddings = BertEmbeddings.pretrained(name='bert_base_cased', lang='en') \\\n",
        "        .setInputCols(['document', 'token']) \\\n",
        "        .setOutputCol('embeddings')\n",
        "\n",
        "ner_model = NerDLModel.pretrained(MODEL_NAME, 'en') \\\n",
        "    .setInputCols(['document', 'token', 'embeddings']) \\\n",
        "    .setOutputCol('ner')\n",
        "\n",
        "ner_converter = NerConverter() \\\n",
        "    .setInputCols(['document', 'token', 'ner']) \\\n",
        "    .setOutputCol('ner_chunk')\n",
        "\n",
        "nlp_pipeline = Pipeline(stages=[\n",
        "    documentAssembler, \n",
        "    tokenizer,\n",
        "    embeddings,\n",
        "    ner_model,\n",
        "    ner_converter\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv0abcwhXWC-"
      },
      "source": [
        "## 6. Run the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYf_9sXDXR4t"
      },
      "outputs": [],
      "source": [
        "empty_df = spark.createDataFrame([['']]).toDF('text')\n",
        "pipeline_model = nlp_pipeline.fit(empty_df)\n",
        "df = spark.createDataFrame(pd.DataFrame({'text': text_list}))\n",
        "result = pipeline_model.transform(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQY8tAP6XZJL"
      },
      "source": [
        "## 7. Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "B0P1hm_D3gGx",
        "outputId": "0f031ef0-ec8c-47bb-958f-0c40566cd114"
      },
      "outputs": [],
      "source": [
        "from sparknlp_display import NerVisualizer\n",
        "\n",
        "NerVisualizer().display(\n",
        "    result = result.collect()[0],\n",
        "    label_col = 'ner_chunk',\n",
        "    document_col = 'document'\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of NER_EN.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "45150093197569bb3a58481dcd32cd1adb45462fa3448719e8ac38ada6166aca"
    },
    "kernelspec": {
      "display_name": "Python 3.6.10 64-bit ('tensorflow2_p36': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
